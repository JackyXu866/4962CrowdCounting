# -*- coding: utf-8 -*-
"""JHUProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kXJre4Zq-uZ2fArVGyLimXkvgu7xdYc4

### JHU-CROWD++ Processing to PyTorch Dataset

Dataset Source: http://www.crowd-counting.com/

Note: in this ground-truth files, there appear to be points for the heads that are not inside the image. For the purposes of this project, such points with coordinates exceeding the images' borders will have their coordinates clipped to fit in the image. This ensures we do not lose any datapoints while running our code.
"""

# import required libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import random

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils

import IPython
import IPython.display

import warnings

warnings.filterwarnings("ignore")

import os
import zipfile

import PIL
from PIL import Image

import cv2

import numbers
import collections

# Custom transforms implementations credited to
# https://github.com/mlagunas/pytorch-nptransforms/blob/master/np_transforms.py

def _is_numpy_image(img):
    return isinstance(img, np.ndarray)

def crop(pic, i, j, h, w):
    if not _is_numpy_image(pic):
        raise TypeError('img should be Numpy Image. Got {}'.format(type(pic)))

    return pic[i:i + h, j:j + w, :]

class RandomCrop(object):
    """
    Performs a random crop in a given numpy array using only the first two dimensions (width and height)
    """

    def __init__(self, size, ):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size

    @staticmethod
    def get_params(pic, output_size):

        # read dimensions (width, height, channels)
        w, h, c = pic.shape

        # read crop size
        th, tw = output_size

        # get crop indexes
        i = random.randint(0, w - tw)
        j = random.randint(0, h - th)

        return i, j, th, tw

    def __call__(self, pic):
        """
        :param input: numpy array
        :return: numpy array croped using self.size
        """

        # check type of [pic]
        if not _is_numpy_image(pic):
            raise TypeError('img should be numpy array. Got {}'.format(type(pic)))

        # if image has only 2 channels make it three channel
        if len(pic.shape) != 3:
            pic = pic.reshape(pic.shape[0], pic.shape[1], -1)

        # get crop params: starting pixels and size of the crop
        i, j, th, tw = self.get_params(pic, self.size)

        # perform cropping and return the new image
        return pic[i:i + th, j:j + tw, :]

class Normalize_01(object):
    """
    Normalize the values of a numpy array between 0-1
    """

    def __init__(self, min=None, max=None):
        """
        :param min: minimum value, by default None. Useful to normalize 0-1 globally
               max: maximum value, by default None. Useful to normalize 0-1 globally
        """
        self.min = min
        self.max = max

    def __call__(self, pic):
        """
        :param pic: numpy array
        :return: same array with its values normalized between 0-1
        """
        min = self.min if self.min is not None else np.min(pic)
        max = self.max if self.max is not None else np.max(pic)

        # check type of [pic]
        if not _is_numpy_image(pic):
            raise TypeError('img should be numpy array. Got {}'.format(type(pic)))
        pic = (pic - min) / (max - min)
        return pic

class ToTensor(object):
    """
    Convert a ``numpy.ndarray`` to tensor.
    """

    def __call__(self, pic):
        """
        Args:
            converts pic (numpy array) to Tensor
        Returns:
            Tensor: Converted image.
        """

        # check type of [pic]
        if not _is_numpy_image(pic):
            raise TypeError('img should be numpy array. Got {}'.format(type(pic)))

        if len(pic.shape) == 1: return torch.FloatTensor(pic.copy())

        return torch.FloatTensor(pic.transpose((2, 0, 1)).copy())

class Scale(object):
    """
    Rescale the given numpy image to a specified size.
    """

    def __init__(self, size, interpolation="bilinear"):
        assert isinstance(size, int) or (isinstance(size, collections.abc.Iterable) and len(size) == 2)
        self.size = size
        self.interpolation = interpolation

    def __call__(self, pic):

        # check type of [pic]
        if not _is_numpy_image(pic):
            raise TypeError('img should be numpy array. Got {}'.format(type(pic)))

        if isinstance(self.size, int):
            # if size is specified with one dimension only get the second one keeping the
            # aspect-ratio

            # get the size of the original image
            w, h = pic.shape[:2]
            if (w <= h and w == self.size) or (h <= w and h == self.size):
                return pic

            # calculate the ouput size keeping the aspect-ratio
            if w < h:
                ow = self.size
                oh = int(self.size * h / w)
            else:
                oh = self.size
                ow = int(self.size * w / h)

            # create the output array
            img_out = np.zeros((ow, oh, pic.shape[2]))

            if len(pic.shape) == 3:
                # if 3D image, scale each channel individually
                for i in range(pic.shape[2]):
                    img_out[:, :, i] = np.array(Image.fromarray(pic[:, :, i]).resize((ow, oh)))
                return img_out
            else:
                # if 2D image, scale image
                return np.array(Image.fromarray(pic).resize((ow, oh)))
        else:
            # if size is specified with 2 dimensions apply the scale directly
            # create the output array

            if len(pic.shape) == 3:
                img_out = np.zeros((self.size[0], self.size[1], pic.shape[2]))

                # if 3D image, scale each channel individually
                for i in range(pic.shape[2]):
                    img_out[:, :, i] = np.array(Image.fromarray(pic[:, :, i]).resize(self.size))
                return img_out
            else:
                # if 2D image, scale image
                return np.array(Image.fromarray(pic).resize(self.size))

# Dataset object for this data
class JHUCrowdDataset(Dataset):
    def __init__(self, gt_dir, img_dir, transform=None):
        self.gt_dir = gt_dir
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(os.listdir(self.img_dir))

    def __getitem__(self, idx):
        # Debug: print gt file name (matches image file name)
        # print(sorted(os.listdir(self.gt_dir))[idx])
        gt_path = os.path.join(self.gt_dir, sorted(os.listdir(self.gt_dir))[idx])
        img_path = os.path.join(self.img_dir, sorted(os.listdir(self.img_dir))[idx])

        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)
        labels = self.getCoords(gt_path, image.shape[0:2])

        if self.transform:
            image = self.transform(image)
            labels = self.transform(labels)
        return (image, labels)
    
    # Return an np array with all the heads' coordinates
    # Storing the ground truths in an array makes transforms easier
    def getCoords(self, gt_path, image_shape):
        faces = np.zeros(image_shape)
        with open(gt_path,'r') as data_file:
          for line in data_file:
              data = line.split()
              data_y = min(int(data[1]), faces.shape[0] - 1)
              data_x = min(int(data[0]), faces.shape[1] - 1)
              # if data_y > faces.shape[0]:
              #     #data_y = faces.shape[0]
              #     continue
              # elif data_x > faces.shape[1]:
              #     #data_x = faces.shape[1]
              #     continue
              faces[data_y, data_x] = 1
        return faces

if __name__ == "__main__":
    print('Program start.')

    base_dir = '.\\jhu_crowd_v2.0'
    train_dir = os.path.join(base_dir, 'train')
    val_dir = os.path.join(base_dir, 'val')
    test_dir = os.path.join(base_dir, 'test')
    print(os.listdir(train_dir))

    # Apply transforms
    transformer = transforms.Compose([
        Scale(size=(512,512)),
        RandomCrop(size=(512,512)),
        Normalize_01(),
        ToTensor()
    ])
    print('Autobots, roll out!')

    # Get directories
    train_gt_dir = os.path.join(train_dir, 'gt')
    train_img_dir = os.path.join(train_dir, 'images')
    val_gt_dir = os.path.join(val_dir, 'gt')
    val_img_dir = os.path.join(val_dir, 'images')
    test_gt_dir = os.path.join(test_dir, 'gt')
    test_img_dir = os.path.join(test_dir, 'images')
    print('Directories found.')

    # Create datasets
    train_data = JHUCrowdDataset(train_gt_dir, train_img_dir, transform=transformer)
    val_data = JHUCrowdDataset(val_gt_dir, val_img_dir, transform=transformer)
    test_data = JHUCrowdDataset(test_gt_dir, test_img_dir, transform=transformer)
    print('Datasets built.')

    # Create subsets of agreed-upon sizes, comment to use the full dataset
    train_subset = torch.utils.data.Subset(train_data, range(500))
    val_subset = torch.utils.data.Subset(train_data, range(100))
    test_subset = torch.utils.data.Subset(train_data, range(200))
    print('Subsets built.')

    train_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)
    val_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)
    test_dataloader = DataLoader(train_subset, batch_size=64, shuffle=True)
    print('DataLoaders built.')

    # Run through a batch to confirm it loads properly
    for i, data in enumerate(train_dataloader):
        print(data[0].shape)
        print(data[1].shape)
        break
    print('Batch loaded.')

    # Visualize one of the images
    fig = plt.figure(figsize=(10, 10))

    # for i in range(len(train_dataset)):
    i = 36
    sample = train_data[i]

    print(sample[0].shape, sample[1].shape)

    ax = plt.subplot(1, 1, 1)
    plt.tight_layout()
    ax.set_title('Sample #{}'.format(i))
    ax.axis('off')

    plt.imshow(sample[0][0,:,:], cmap='gray')
    plt.imshow(sample[1][0,:,:], alpha=0.5)
    plt.show()
    print('Image visualized.')